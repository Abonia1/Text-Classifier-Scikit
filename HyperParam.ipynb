{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neocase Case Category HyperParameter Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pipeline for text feature extraction and evaluation\n",
    "\n",
    "The dataset used here is the neocase case dataset which will be automatically downloaded and then cached and reused for the document classification example.\n",
    "\n",
    "You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 12 of them.\n",
    "\n",
    "Here is a sample output of a run on a quad-core machine::\n",
    "\n",
    "Loading 12 Category dataset for categories: ['ABSENCES','AVANTAGES SOCIETE GENERALE',\"DEPART DE L'ENTREPRISE\",'OUTILS RH','PAIE, REMUNERATION ET FRAIS PROFESSIONNELS','THEME AUTRE','DONNEES PERSONNELLES','FORMATION','INTERIM  STAGES  EXTERNES','MUTUELLE  PREVOYANCE','PARCOURS PROFESSIONNEL','TEMPS DE TRAVAIL'] 556documents 12 categories\n",
    "Performing grid search... pipeline: ['vect', 'tfidf', 'clf'] parameters: {'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07), 'clf__max_iter': (10, 50, 80), 'clf__penalty': ('l2', 'elasticnet'), 'tfidf__use_idf': (True, False), 'vect__max_n': (1, 2), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000)} done in 1737.030s\n",
    "\n",
    "Best score: 1.000 Best parameters set: clf__alpha:1e-05  clf__max_iter: 5 clf__penalty: 'elasticnet' tfidf__use_idf: True vect__max_n: 2 vect__max_df: 1.0 vect__max_features: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sklearn \n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import document \n",
    "from sklearn import datasets \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories=['ABSENCES','AVANTAGES SOCIETE GENERALE',\"DEPART DE L'ENTREPRISE\",'OUTILS RH','PAIE, REMUNERATION ET FRAIS PROFESSIONNELS','THEME AUTRE','DONNEES PERSONNELLES','FORMATION','INTERIM  STAGES  EXTERNES','MUTUELLE  PREVOYANCE','PARCOURS PROFESSIONNEL','TEMPS DE TRAVAIL']\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading neocase Case dataset for categories:\n",
      "['ABSENCES', 'AVANTAGES SOCIETE GENERALE', \"DEPART DE L'ENTREPRISE\", 'OUTILS RH', 'PAIE, REMUNERATION ET FRAIS PROFESSIONNELS', 'THEME AUTRE', 'DONNEES PERSONNELLES', 'FORMATION', 'INTERIM  STAGES  EXTERNES', 'MUTUELLE  PREVOYANCE', 'PARCOURS PROFESSIONNEL', 'TEMPS DE TRAVAIL']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading neocase Case dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 documents\n",
      "12 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_to_train = sklearn.datasets.load_files(\"document\",categories=categories)\n",
    "\n",
    "\n",
    "print(\"%d documents\" % len(docs_to_train.filenames))\n",
    "print(\"%d categories\" % len(docs_to_train.target_names))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__max_iter': (5,), 'clf__alpha': (1e-05, 1e-06), 'clf__penalty': ('l2', 'elasticnet')}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 42.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2557.651s\n",
      "\n",
      "Best score: 1.000\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__max_iter: 5\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    print(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(docs_to_train.data, docs_to_train.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
